// data
dataset = dataset:mnist
    data_path = "../../mnist"

split = split:cv_111
    n_splits = 5
    val_size = 10000
    dataset = dataset

batch_iter_factory = batch_iter_factory:fin
    get_batch_iter = batch_iter:simple
        @lazy
        ids = train_ids
        load_x = load_x
        load_y = load_y
        batch_size = batch_size
        shuffle = true

batch_predict = batch_predict:simple()

// model

T = meta:import
    module = "torch"

model_core = model_core:proto()

logits2pred = torch:softmax

model = model:torch
    model_core = model_core
    logits2pred = logits2pred
    logits2loss = torch:softmax_cross_entropy
    optimize = T.optim.Adam

train = train:proto
    @lazy
    atol = 0.01
    lr_dec_mul = 0.7
    lr_init = 0.1
    n_epochs = n_epochs
    patience = 5
    rtol = 0.03
    log_path = console.log_path

    load_x = load_x
    load_y = load_y

    model = model
    train_batch_iter_factory = batch_iter_factory
    val_ids = val_ids
    batch_predict = batch_predict

// commands

train_model = command:train_model
    train = train
    model = model
    save_model_path = console.save_model_path
    restore_model_path = console.restore_model_path

build_experiment = experiment:flat
    makefile = "proto"
    config_path = console.config_path
    experiment_path = console.experiment_path
    split = split

// misc

batch_size = 2000
n_epochs = 40

load_x = dataset.load_x
load_y = dataset.load_y

console = io:console()

train_ids = io:json
    path = console.train_ids_path
val_ids = io:json
    path = console.val_ids_path
ids = io:json
    path = console.ids_path
